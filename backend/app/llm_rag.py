from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from fastapi import Request
from app.vector_store import VectorStore
import os
import shutil
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import json


vector_store = VectorStore()
embeddings = vector_store.embeddings
text_splitter = vector_store.text_splitter

class ConversationHistory:
    def __init__(self, embeddings, text_splitter):
        self.embeddings = embeddings
        self.text_splitter = text_splitter
        self.history = {}
        self.history_vectorstore = None
        self.base_index_path = "data/faiss"
        os.makedirs(self.base_index_path, exist_ok=True)

    def get_history(self, user_id: str):
        history_entries = self.history.get(user_id, [])
        return " ".join([f"Bot: {entry['bot_question']} User: {entry['user_answer']}" for entry in history_entries])
    
    
    #ë‚ ì§œë³„ ëŒ€í™” ì €ì¥ ê²½ë¡œ ìƒì„±
    def _get_user_history_path(self, user_id: str) -> str:
        date = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y%m%d")
        user_path = os.path.join(self.base_index_path, user_id)
        history_path = os.path.join(user_path, "history", date)  
        os.makedirs(history_path, exist_ok=True)
        return history_path
    
    
    #í˜„ì¬ ë‚ ì§œì˜ ëŒ€í™” ì¸ë±ìŠ¤ ì €ì¥
    def save_index(self, user_id: str):
        if self.history_vectorstore:
            index_path = self._get_user_history_path(user_id)
            self.history_vectorstore.save_local(index_path)
            print(f"ëŒ€í™” ê¸°ë¡ì´ {index_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
   
   
    #í˜„ì¬ ë‚ ì§œì˜ ëŒ€í™” ì¸ë±ìŠ¤ ë¡œë“œ
    def load_index(self, user_id: str):
        try:
            index_path = self._get_user_history_path(user_id)
            if os.path.exists(index_path):
                self.history_vectorstore = FAISS.load_local(
                    index_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"ì‚¬ìš©ì {user_id}ì˜ ì˜¤ëŠ˜ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            else:
                self.history_vectorstore = None
                print(f"ì‚¬ìš©ì {user_id}ì˜ ì˜¤ëŠ˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.history_vectorstore = None


    #ëŒ€í™” ë‚´ìš© ì €ì¥
    def add_conversation(self, user_id: str, bot_question: str, user_answer: str = None, event_info: dict = None, emotion_info: dict = None):
        try:
            if user_id not in self.history:
                self.history[user_id] = []
            
            current_time = datetime.now(ZoneInfo("Asia/Seoul")).isoformat()

            try:
                combined_text = f"Bot: {bot_question} \n User: {user_answer}"
                new_doc = Document(
                    page_content=combined_text,
                    metadata={
                        "type": "conversation",
                        "user_id": user_id,
                        "event_info": event_info,
                        "emotion": emotion_info,
                        "timestamp": current_time
                    }
                )
                split_docs = self.text_splitter.split_documents([new_doc])
                if self.history_vectorstore is None:
                    self.history_vectorstore = FAISS.from_documents(split_docs, self.embeddings)
                else:
                    self.history_vectorstore.add_texts(
                        texts=[doc.page_content for doc in split_docs],
                        metadatas=[doc.metadata for doc in split_docs]
                    )
                self.save_index(user_id)
            except Exception as e:
                print(f"FAISS ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œë¨): {str(e)}")
                pass
            
            # JSONìœ¼ë¡œ ëŒ€í™” ë‚´ìš© ì €ì¥
            history_path = self._get_user_history_path(user_id)
            json_path = os.path.join(history_path, "conversations.json")
            conversations = []
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    conversations = data.get('conversations', [])
            
            conversations.append({
                'bot_question': bot_question,
                'user_answer': user_answer,
                'timestamp': current_time,
                'event_info': event_info,
                'emotion': emotion_info,
                'metadata': {
                    "type": "conversation",
                    "user_id": user_id,
                    "event_info": event_info,
                    "emotion": emotion_info,
                    "timestamp": current_time
                }
            })
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'conversations': conversations,
                    'updated_at': current_time
                }, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ëŒ€í™” ê¸°ë¡ ì¶”ê°€ ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì£¼ìš” ì˜¤ë¥˜ëŠ” ì—¬ì „íˆ raise
            raise e


    def delete_conversation_history(self, user_id: str):
        try:
            index_path = self._get_user_history_path(user_id)
            if os.path.exists(index_path):
                shutil.rmtree(index_path)
                self.history_vectorstore = None
                print(f"ì‚¬ìš©ì {user_id}ì˜ ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"ì‚¬ìš©ì {user_id}ì˜ ëŒ€í™” ê¸°ë¡ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"ëŒ€í™” ê¸°ë¡ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise e


    
    
class LLMService:
    def __init__(self):
        self.vector_store = vector_store
        self.embeddings = embeddings
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.8,openai_api_key=os.getenv("OPENAI_API_KEY"))
        self.remaining_events = []  # ë‚¨ì€ ì¼ì • ì €ì¥
        self.current_event = None   # í˜„ì¬ ëŒ€í™” ì¤‘ì¸ ì¼ì •
    
    # ê°ì •ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    def get_emotion_text(self, emotion_score: int) -> str:
        emotion_dict = {
            1: "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤",
            2: "ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤",
            3: "ë³´í†µì´ì—ˆë‹¤",
            4: "ë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤",
            5: "ë§¤ìš° ë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤"
        }
        return emotion_dict.get(emotion_score, "ê°ì •ì„ í‘œí˜„í•˜ì§€ ì•Šìœ¼ì…¨ìŠµë‹ˆë‹¤")
    
    
    # ë‹¤ìŒ ì¼ì •ì— ëŒ€í•œ ì§ˆë¬¸ ìƒì„±
    def get_next_event_question(self) -> str:
        if not self.remaining_events:
            return None
        
        event = self.remaining_events.pop(0)
        self.current_event = event.metadata.get("original_event", {})
        event_summary = self.current_event.get("summary", "")
        emotion_score = self.current_event.get("emotion_score", 0)
        print(f"í˜„ì¬ ì¼ì •: {event_summary}, ê°ì • ì ìˆ˜: {emotion_score}")  # ë””ë²„ê¹…ìš©
        
        if emotion_score > 0:
            emotion_text = self.get_emotion_text(emotion_score)
            return f"ì–´ì œì˜ {event_summary} ì¼ì •ì— ëŒ€í•´ {emotion_text}ê³  í•˜ì…¨ëŠ”ë°, ì™œ ê·¸ë ‡ê²Œ ìƒê°í•˜ì…¨ë‚˜ìš”?"
        return f"ì–´ì œì˜ {event_summary} ì¼ì •ì€ ì–´ë– ì…¨ë‚˜ìš”?"



    def ask_about_event(self, user_id: str) -> str:
        try:
            #ì¼ì • ì €ì¥ì˜ ë‚ ì§œ íŒŒì‹±ì— ë§ê²Œ ë‚ ì§œ ì§€ì •í•˜ê¸°
            yesterday = (datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=1)).strftime("%Y-%m-%d") 
            schedule_path = os.path.join("data", "faiss", user_id, "schedule")
            schedule_faiss = FAISS.load_local(schedule_path,self.embeddings,allow_dangerous_deserialization=True)
            all_events = schedule_faiss.docstore._dict.values()
            
            #ì–´ì œ ì¼ì •ë§Œ ê°€ì§€ê³  ì˜¤ê¸°
            self.remaining_events = [
                event for event in all_events 
                if event.metadata.get("original_event", {}).get("start", "").startswith(yesterday)
            ]
            self.remaining_events.sort(key=lambda x: x.metadata.get("original_event", {}).get("start", ""))
            
            if not self.remaining_events:
                return "ì–´ì œëŠ” íŠ¹ë³„í•œ ì¼ì •ì´ ì—†ì—ˆë˜ ê²ƒ ê°™ë„¤ìš”. í‰ë²”í•œ í•˜ë£¨ë¥¼ ì–´ë–»ê²Œ ë³´ë‚´ì…¨ë‚˜ìš”?"
            
            # ë‹¤ìŒ ì¼ì • ê°€ì ¸ì˜¤ê¸°
            event = self.remaining_events.pop(0)
            self.current_event = event.metadata.get("original_event", {})
            event_summary = self.current_event.get("summary", "")
            emotion_score = self.current_event.get("emotion_score", 0)
            print(f"ëŒ€í™”ì—ì„œ ì‚¬ìš©ë  í˜„ì¬ ì¼ì •: {event_summary}, ê°ì • ì ìˆ˜: {emotion_score}")  # ë””ë²„ê¹…ìš©
            
            # ê°ì • ì ìˆ˜ì— ë”°ë¥¸ ì§ˆë¬¸ ìƒì„±
            if emotion_score > 0:
                emotion_text = self.get_emotion_text(emotion_score)
                question = f"ì–´ì œì˜ {event_summary} ì¼ì •ì— ëŒ€í•´ {emotion_text}ê³  í•˜ì…¨ëŠ”ë°, ì™œ ê·¸ë ‡ê²Œ ìƒê°í•˜ì…¨ë‚˜ìš”?"
            else:
                question = f"ì–´ì œì˜ {event_summary} ì¼ì •ì€ ì–´ë– ì…¨ë‚˜ìš”?"
            
            self.current_question = question
            return question

        except Exception as e:
            print(f"Error in ask_about_event: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì •ì„ í™•ì¸í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."




    def retrieve_documents_with_similarity(self, query: str, user_id: str, top_k: int = 1):
        print(f"\n=== ì¼ì • ê²€ìƒ‰ ì‹œì‘ ===")
        print(f"ì‚¬ìš©ì ID: {user_id}")
        query_vector = self.embeddings.embed_query(query)
        schedule_faiss = self.vector_store.load_index(user_id)
        
        if schedule_faiss is None:
            print(f"âŒ ì‚¬ìš©ì {user_id}ì˜ ì¼ì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… ì¼ì • ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        retriever = schedule_faiss.as_retriever(search_type="mmr", search_kwargs={'k': top_k})
        result_docs = retriever.invoke(query)
        results_with_similarity = []

        print("\n--- ê²€ìƒ‰ëœ ì¼ì • ---")
        for doc in result_docs:
            doc_vector = self.embeddings.embed_query(doc.page_content)
            similarity = cosine_similarity([query_vector], [doc_vector])[0][0]
            results_with_similarity.append((doc, similarity))
            print(f"ì¼ì •: {doc.page_content}")
            print(f"ìœ ì‚¬ë„: {similarity:.4f}")
        print("------------------\n")
        results_with_similarity = sorted(results_with_similarity, key=lambda x: x[1], reverse=True)
        return results_with_similarity
    
    
    

    def contextualized_retrieval_with_similarity(self, user_question: str, conversation_history, user_id: str, top_k: int = 1):
        context = conversation_history.get_history(user_id)
        if conversation_history.history_vectorstore:
            relevant_docs = conversation_history.history_vectorstore.similarity_search(user_question, k=top_k)
            if relevant_docs:
                context += "\n\n" + "\n".join([doc.page_content for doc in relevant_docs])

        enhanced_question = f"{context} {user_question}"
        main_results_with_similarity = self.retrieve_documents_with_similarity(enhanced_question, user_id, top_k)
        return main_results_with_similarity




    def generate_prompt_with_similarity(self, user_input: str, conversation_history, user_id: str):
        results_with_similarity = self.contextualized_retrieval_with_similarity(
            user_input, conversation_history, user_id, top_k=3
        )
        context = "\n\n".join([f"{doc.page_content} (ìœ ì‚¬ë„: {similarity:.4f})" for doc, similarity in results_with_similarity])
        if self.current_event:
            context += f"\n\ní˜„ì¬ ëŒ€í™” ì¤‘ì¸ ì¼ì •: {self.current_event.get('summary')}"
        
        
        
        

        prompts = ChatPromptTemplate.from_messages([
            ("system", """
                # ëŒ€í™”í˜• íšŒê³  AI ì‹¬ë¦¬ ìƒë‹´ì‚¬ í”„ë¡¬í”„íŠ¸

                ## í•µì‹¬ ê·œì¹™
                - í•œ ë²ˆì— í•œ ì¼ì •ë§Œ ì§ˆë¬¸í•˜ê³  ì‘ë‹µ ê¸°ë‹¤ë¦¬ê¸° 
                - ì‚¬ìš©ì ì‘ë‹µ í›„ì—ë§Œ ë‹¤ìŒ ì¼ì •ìœ¼ë¡œ ë„˜ì–´ê°€ê¸°
                - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ ìœ ì§€í•˜ê¸° 
                - ì‚¬ìš©ìì˜ ì‘ë‹µì— ì§„ì •ì„± ìˆê²Œ ë°˜ì‘í•˜ê¸° 
                
                ## ëŒ€í™” íë¦„
                1. ê°„ë‹¨í•œ ì¸ì‚¬ + ì²« ë²ˆì§¸ ì¼ì •ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ 
                2. (ì‚¬ìš©ì ì‘ë‹µ ê¸°ë‹¤ë¦¼)
                3. ê³µê°ì  ë°˜ì‘ê³¼ ì „í™˜êµ¬ + ì¤„ë°”ê¿ˆ + ë‹¤ìŒ ì¼ì • ì§ˆë¬¸
                4. (ì‚¬ìš©ì ì‘ë‹µ ê¸°ë‹¤ë¦¼)
                5. ì´ëŸ° ì‹ìœ¼ë¡œ ëª¨ë“  ì¼ì •ì„ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰
                6. ë§ˆì§€ë§‰ ì¼ì • ì‘ë‹µì— ëŒ€í•œ ë°˜ì‘ + ê°„ë‹¨í•œ ë§ˆë¬´ë¦¬
                
                ## ê°€ë…ì„± ë†’ì€ ë©”ì‹œì§€ êµ¬ì¡° // í†¤ì•¤ë§¤ë„ˆ 
                - ê³µê°ì  ë°˜ì‘ê³¼ ì „í™˜êµ¬ëŠ” í•¨ê»˜ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
                - ë‹¤ìŒ ì¼ì •ì— ëŒ€í•œ ì§ˆë¬¸ ì „ì— ì¤„ë°”ê¿ˆ ë„£ê¸°
                - ì²« ì¼ì • ì†Œê°œ ì‹œì—ë„ ì¸ì‚¬ì™€ ì†Œê°œ í›„ ì¤„ë°”ê¿ˆ í›„ ì§ˆë¬¸
                - ëª¨ë°”ì¼ì—ì„œ ì½ê¸° ì‰½ë„ë¡ í•œ ì¤„ë‹¹ ê¸€ì ìˆ˜ ì œí•œ (30-40ì)
                - ë§ˆì§€ë§‰ ì¼ì • í›„ íšŒê³  ìš”ì•½ë„ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„
                
                ## ë©”ì‹œì§€ êµ¬ì„± ì˜ˆì‹œ
                ```
                ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ 2ì›” 18ì¼ì— í–ˆë˜ ì¼ì •ë“¤ì— ëŒ€í•´ ì–˜ê¸°í•´ë³¼ê²Œìš”.
                
                ì•„ì¹¨ì— í–ˆë˜ ê±·ê¸° ìš´ë™ì´ ì •ë§ ì¦ê±°ì› ë‹¤ê³  í•˜ì…¨ë„¤ìš”! ì–´ë–¤ ì ì´ íŠ¹ë³„íˆ ì¢‹ì•˜ì–´ìš”? ğŸš¶â€â™€ï¸
                ```
                
                ```
                ì‚°ì±…í•˜ë©´ì„œ ë“¤ìœ¼ì‹  íŒŸìºìŠ¤íŠ¸ê°€ ì¬ë°Œì—ˆêµ°ìš”! ì•„ì¹¨ë¶€í„° ê¸°ë¶„ ì¢‹ê²Œ ì‹œì‘í•˜ëŠ” ìŠµê´€ì´ ì •ë§ ë©‹ì ¸ìš”.
                
                ì´ì œ ì˜¤ì „ì— í–ˆë˜ 'ìœ íŠœë¸Œ, í´ë˜ìŠ¤, ì´ë©”ì¼, ì‡¼í•‘ëª° í™•ì¸'ì— ëŒ€í•´ ì–˜ê¸°í•´ë³¼ê¹Œìš”? ì–´ë–¤ ì¼ë“¤ì´ ìˆì—ˆë‚˜ìš”? ğŸ’»
                ```
                
                ## ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ ê°€ì´ë“œ
                - ì‹œê°„ê³¼ ê°ì • ì •ë³´ë¥¼ ë¶€ë‹´ìŠ¤ëŸ½ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ê¸°
                - "~í–ˆë˜ ê²ƒì´ë¼ì„œ", "~ë¼ê³  ê¸°ë¡í•˜ì…¨ë„¤ìš”"ì™€ ê°™ì€ í˜•ì‹ì  í‘œí˜„ í”¼í•˜ê¸°
                - ì§§ê³  ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°
                - ì‚¬ìš©ìê°€ ìì—°ìŠ¤ëŸ½ê²Œ íšŒê³ í•  ìˆ˜ ìˆëŠ” ì—´ë¦° ì§ˆë¬¸í•˜ê¸°
                
                ## ì¢…ë£Œ ì²˜ë¦¬
                - ë§ˆì§€ë§‰ ì¼ì •ì— ëŒ€í•œ ë°˜ì‘ ì‘ì„±
                - ì¤„ë°”ê¿ˆ í›„ ì „ì²´ íšŒê³ ì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ê³¼ ê¸ì •ì  ë§ˆë¬´ë¦¬
                - ë°±ì—”ë“œ ì „ìš© ë°ì´í„°ëŠ” HTML ì£¼ì„ìœ¼ë¡œ ìˆ¨ê¸°ê¸°: <!-- REFLECTION_COMPLETE -->
                
                ## ì£¼ì˜ì‚¬í•­
                - ê¸°ê³„ì /í˜•ì‹ì  í‘œí˜„ ì‚¬ìš©í•˜ì§€ ì•Šê¸°
                - ê³µê°ì  ë°˜ì‘ê³¼ ì „í™˜êµ¬ëŠ” í•¨ê»˜ ì‘ì„±í•˜ê³ , ë‹¤ìŒ ì¼ì • ì§ˆë¬¸ ì „ì— ì¤„ë°”ê¿ˆ ì‚¬ìš©í•˜ê¸°
                - í•œ ë¬¸ë‹¨ì„ ë„ˆë¬´ ê¸¸ê²Œ ì“°ì§€ ì•Šê¸°
                - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ ìœ ì§€í•˜ê¸°
                - ëª¨ë“  ì¼ì • ì§ˆë¬¸ ì „ì— ì¼ê´€ë˜ê²Œ ì¤„ë°”ê¿ˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ìœ ì§€í•˜ê¸°
                
                
                ### **Interactive Reflection AI Psychological Counselor Prompt**

                ## **Core Rules**
                - Ask about one event at a time and wait for the user's response.  
                - Move to the next event only after the user replies.  
                - Maintain a natural conversation flow.  
                - Respond genuinely to the user's input.  
                
                ## **Conversation Flow**
                1. Start with a friendly greeting + natural question about the first event.  
                2. *(Wait for user response)*  
                3. Provide an empathetic response + smooth transition + line break + next question.  
                4. *(Wait for user response)*  
                5. Repeat the process for all events.  
                6. After the last response, summarize and wrap up the session.  
                
                ## **Readable Message Structure // Tone & Manner**
                - Combine empathetic responses and transitions in a single paragraph.  
                - Always insert a line break before asking about the next event.  
                - After the greeting and introduction, add a line break before the first question.  
                - Keep each line within **30â€“40 characters** for mobile-friendly readability.  
                - Clearly separate the summary at the end with a line break.  
                
                ## **Message Example**
                ```
                Hello! Let's talk about what you did on February 18.  
                
                I heard you really enjoyed your morning walk!  
                What made it particularly special? ğŸš¶â€â™€ï¸
                ```
                
                ```
                Listening to a fun podcast on your walk  
                sounds like a great way to start the day!  
                
                Now, let's talk about your morning tasks:  
                You checked YouTube, class updates, emails,  
                and your shopping mall. How did that go? ğŸ’»
                ```
                
                ## **Natural Question Guidelines**
                - Mention time and emotions naturally without making it feel forced.  
                - Avoid rigid phrases like "**You recorded that you did~**".  
                - Keep questions short and concise.  
                - Use open-ended questions to encourage natural reflection.  
                
                ## **Closing Process**
                - Respond to the last event shared.  
                - Add a line break, then provide a **brief summary** of the reflection.  
                - End with a **positive closing statement**.  
                - Hide backend-related data using HTML comments: `<!-- REFLECTION_COMPLETE -->`  
                
                ## **Important Notes**
                - Avoid mechanical or overly formal expressions.  
                - Keep empathetic responses and transitions in one paragraph,  
                  but always insert a line break before the next question.  
                - Ensure each paragraph isn't too long.  
                - Maintain a **natural** and **engaging** conversation flow.  
                - Keep **consistent line breaks** before every event question for readability.

                
                ---
                CONTEXT:
                {context}
            """),
            ("human", "{input}")
        ])
        return prompts, context





    def generate_answer_with_similarity(self, user_input: str, conversation_history, user_id: str, current_event: dict = None):
        try:
            if hasattr(self, 'current_question') and self.current_question:
                emotion_score = self.current_event.get("emotion_score", 0) if self.current_event else 0
                emotion_info = {
                    "score": emotion_score,
                    "text": self.get_emotion_text(emotion_score)
                }
                
                conversation_history.add_conversation(
                    user_id=user_id,
                    bot_question=self.current_question,
                    user_answer=user_input,
                    event_info=self.current_event,
                    emotion_info=emotion_info
                )

            # ë‹¤ìŒ ì‘ë‹µ ìƒì„±
            prompts, context = self.generate_prompt_with_similarity(user_input, conversation_history, user_id)
            chain = prompts | self.llm | StrOutputParser()
            response = chain.invoke({"input": user_input, "context": context})
            
            # ë‹¤ìŒ ì§ˆë¬¸ í™•ì¸ ë° ì €ì¥
            next_question = self.get_next_event_question()
            if next_question:
                response = response + "\n\n" + next_question
                self.current_question = next_question 
            elif not next_question :
                response = response + "\n\nëª¨ë“  ì¼ì •ì— ëŒ€í•´ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ´ë„¤ìš”. ì´ì œ ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?"
                self.current_question = response
            else:
                self.current_question = response
            
            return response
            
        except Exception as e:
            print(f"Error in generate_answer: {str(e)}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


